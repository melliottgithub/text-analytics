{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this text classification project/hmw, I began by establishing a solid foundation with preprocess.py, where I took charge of downloading and initial data handling using scikit-learn. My focus was on cleaning the text data, where I implemented tokenization, the removal of stop words and other methods which are essential steps for natural language processing. I then saved this preprocessed data as pickle files to streamline future access and processing.\n",
    "\n",
    "Moving into the exploratory phase with bonus_eda.ipynb, I conducted a thorough exploratory data analysis. Here, I generated word clouds and frequency distributions to uncover the most common words in the dataset. I also created bar plots to visualize the distribution of articles across the various newsgroups, gaining insights into the dataset's balance and composition.\n",
    "\n",
    "Next, I crafted features.ipynb to build a robust feature set. In this step, I constructed a corpus and created a pipeline for text vectorization, specifically employing the TF-IDF technique. I saved different transformers corresponding to varying vocabulary sizes, allowing me to explore the impact of feature dimensionality on model performance.\n",
    "\n",
    "With training.ipynb, I dove into the heart of machine learning(model training). I prepared vectorized features from the training data and trained several models, saving each one. This modular approach enabled me to assess the efficacy of different algorithms and configurations efficiently.\n",
    "\n",
    "I wrapped up my project with evaluation.ipynb, where I put my models to the test against unseen data. I vectorized the testing data and made predictions, initially gauging the models' performance through accuracy scores. This step was pivotal in assessing the generalizability of the models I had developed.\n",
    "\n",
    "Throughout the project, I made sure to follow good machine learning practices, carefully planning and carrying out each step with attention to detail. I kept my work organized and ensured that everything could be easily replicated, so it is well understood by me for future projects and by anyone else who reads it. This lays a strong foundation for any updates or changes I might need to make later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my text classification project, I began by thoroughly preprocessing the 20 Newsgroups dataset. My strategy was straightforward: clean the text data, tokenize it, and remove all stopwords. This preparation was essential, leading to a streamlined set of data ready for the machine learning models to come. I used TF-IDF vectorization to engineer features, working with vocabularies of varying sizes, like 25,000, 50,000, and 100,000 terms.\n",
    "\n",
    "As I trained the models, I discovered that the LinearSVC model was best, achieving an accuracy of about 0.69 with the 100,000 term vocabulary and training in just under 3 seconds. This was in a big distinction to the MLPClassifier, which had a good but lower accuracy and took significantly longer, approximately 7,339 seconds, to train on the same feature set.\n",
    "\n",
    "The value of a comprehensive EDA became clear to me later in the project. It revealed frequent terms such as \"think\" and \"people\" that dominated the dataset, along with the uneven distribution of articles across the categories. These insights suggested areas for improvement in preprocessing and highlighted the importance of class balance for effective model training.\n",
    "\n",
    "Moving forward, I see a path laid out for refinement. I plan to adjust the preprocessing phase to diminish the influence of overly common terms. Balancing the dataset to represent all categories more equitably could help improve model performance. Additionally, I might consider integrating more sophisticated vectorization methods like word embeddings.\n",
    "\n",
    "Reflecting on the project, I recognize that the outcomes were solid, but there's room for enhancement. The numbers from the EDA have provided a clearer direction for these improvements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
