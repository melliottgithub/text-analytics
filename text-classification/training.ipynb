{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('./text/newsgroups/train.pkl')\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-built feature transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 'trim'\n",
    "transformer = None\n",
    "with open('./models/tfidf_transformer_{}.pkl'.format(VOCABULARY_SIZE), 'rb') as f:\n",
    "    transformer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features from training subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = df_train['tokens'].map(lambda x: ' '.join(x))\n",
    "X_train = transformer.transform(corpus_train).toarray()\n",
    "y_train = df_train['categoryid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_forest_cls(random_state):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 300, 600], \n",
    "        'max_depth': [6, None],\n",
    "        'max_features': ['log2', 'sqrt'],\n",
    "        #'min_samples_split': [2, 4],\n",
    "        #'min_samples_leaf': [1, 2],\n",
    "        #'bootstrap': [True, False],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'random_state': [random_state],\n",
    "    }\n",
    "    grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=2, verbose=1, n_jobs=1)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_mlp_cls(random_state):\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(256, 256)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'max_iter': [1000],\n",
    "        'alpha': [1e-4],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'verbose': [False],\n",
    "        'tol': [1e-4],\n",
    "        'learning_rate_init': [.1],\n",
    "        'random_state': [random_state],\n",
    "    }\n",
    "    grid = GridSearchCV(MLPClassifier(), param_grid, cv=2, verbose=1, n_jobs=1)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "models = {\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=random_state),\n",
    "    'RandomForestClassifier': select_random_forest_cls(random_state=random_state),\n",
    "    'Bagging': BaggingClassifier(random_state=random_state),\n",
    "    'MLPClassifier': select_mlp_cls(random_state=random_state),\n",
    "    'LinearSVC': LinearSVC(random_state=random_state, tol=1e-4, max_iter=5000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    start_time = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    duration = time() - start_time\n",
    "   \n",
    "    file_name = './models/{}_{}.pkl'.format(model_name, str(VOCABULARY_SIZE))\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('{} - {:.2f} secs'.format(file_name, duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
